---
phase: 3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory
plan: 2
type: execute
---

<objective>
Implement embedding service, vector storage, and semantic retrieval for chatbot messages.

Purpose: Build RAG pipeline components (embedding generation, vector storage, retrieval) without modifying chatbot endpoint yet.
Output: Working embedding service, vector store integration, retrieval function that can fetch semantically relevant messages.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-RESEARCH.md
@.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-01-SUMMARY.md

**Previous plan (3.8.4-01) completed:**
- Qdrant Docker container running on VPS
- Collection "chatbot_memory" created (1536 dimensions, Cosine distance)
- RAG dependencies installed (@qdrant/js-client-rest, @langchain packages)
- Multi-tenant payload indexes (organizationId, timestamp)

**Research guidance (from 3.8.4-RESEARCH.md):**

**Don't hand-roll (use libraries):**
- Embedding generation → @langchain/openai OpenAIEmbeddings (NOT manual API calls)
- Conversation chunking → LangChain RecursiveCharacterTextSplitter (NOT fixed-size splits)
- Vector storage → @langchain/qdrant QdrantVectorStore (NOT manual upsert)
- Retrieval → QdrantVectorStore.asRetriever() (NOT manual query logic)

**Chunking strategy:**
- Message-level chunks with 400-token target (~2-3 messages)
- Chunk overlap 50 tokens (12.5%, preserves context across boundaries)
- Format: `role: content` per message for semantic coherence

**Critical implementation notes:**
- ALWAYS include organizationId in Qdrant filters (tenant isolation)
- Use embedDocuments() for batching (not individual embed() calls - rate limiting)
- Store minimal payload (<40KB limit): organizationId, sessionId, timestamp, messageIndex
- Fetch full message content from PostgreSQL after retrieval (not from Qdrant payload)

**Pitfalls to avoid:**
- Rate limit death spiral: Batch embed() calls, use exponential backoff
- Payload size exceeded: Don't store full message content in Qdrant (>40KB fails)
- Embedding model mismatch: Store embeddingModel in payload for versioning
- No tenant filtering: GDPR violation, data leakage between orgs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create embedding service with OpenAI text-embedding-3-small</name>
  <files>packages/server/src/lib/rag/embeddingService.ts (NEW)</files>
  <action>
Create embeddingService.ts using @langchain/openai OpenAIEmbeddings:

**File: packages/server/src/lib/rag/embeddingService.ts**
```typescript
import { OpenAIEmbeddings } from "@langchain/openai";

let embeddingsInstance: OpenAIEmbeddings | null = null;

/**
 * Get singleton OpenAI embeddings instance
 * Model: text-embedding-3-small (1536 dimensions, $0.02/M tokens)
 */
export function getEmbeddings(): OpenAIEmbeddings {
  if (!embeddingsInstance) {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error("OPENAI_API_KEY environment variable required for embeddings");
    }

    embeddingsInstance = new OpenAIEmbeddings({
      model: "text-embedding-3-small",  // 1536 dimensions, 5x cheaper than ada-002
      apiKey: apiKey,
      maxRetries: 3,                     // Retry on rate limits
      timeout: 30000,                    // 30s timeout (embedding can be slow)
    });
  }

  return embeddingsInstance;
}

/**
 * Generate embeddings for conversation messages
 * Uses batching to avoid rate limits (embedDocuments handles this automatically)
 *
 * @param messages Array of formatted messages ("user: content" or "assistant: content")
 * @returns Array of embeddings (1536-dimensional vectors)
 */
export async function embedMessages(messages: string[]): Promise<number[][]> {
  const embeddings = getEmbeddings();

  // LangChain handles batching, rate limiting, retries automatically
  const vectors = await embeddings.embedDocuments(messages);

  return vectors;
}

/**
 * Generate embedding for a single query
 * Use this for retrieval queries, NOT for storing messages (use embedMessages for batch)
 *
 * @param query User's search query
 * @returns Single embedding vector (1536 dimensions)
 */
export async function embedQuery(query: string): Promise<number[]> {
  const embeddings = getEmbeddings();

  // embedQuery method optimized for single queries (vs embedDocuments for batches)
  const vector = await embeddings.embedQuery(query);

  return vector;
}
```

**Environment variable (packages/server/.env.example and VPS .env):**
```
# OpenAI API (for chatbot + embeddings)
OPENAI_API_KEY=sk-...  # Already exists for chatbot, reuse same key
```

**Why embedDocuments() vs manual API calls:**
- Automatic batching (reduces API calls, faster)
- Built-in rate limit handling (retries with exponential backoff)
- Error handling (network failures, timeout, quota exceeded)
- Cost efficiency (batches reduce overhead)

**Token cost estimation:**
- Average message: ~50 tokens
- 100k messages: 5M tokens
- Cost: 5M × $0.02/M = $0.10 ✅ (negligible)
  </action>
  <verify>
    - packages/server/src/lib/rag/embeddingService.ts exists
    - Code uses @langchain/openai OpenAIEmbeddings (not manual fetch to OpenAI API)
    - embedMessages() function returns Promise<number[][]>
    - embedQuery() function returns Promise<number[]>
    - TypeScript compiles without errors (pnpm build in packages/server)
  </verify>
  <done>Embedding service created with OpenAI text-embedding-3-small, batching, and rate limit handling</done>
</task>

<task type="auto">
  <name>Task 2: Implement conversation chunking with LangChain RecursiveCharacterTextSplitter</name>
  <files>packages/server/src/lib/rag/conversationChunker.ts (NEW)</files>
  <action>
Create conversationChunker.ts using LangChain text splitters:

**File: packages/server/src/lib/rag/conversationChunker.ts**
```typescript
import { Document } from "@langchain/core/documents";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { encoding_for_model } from "tiktoken";

interface Message {
  role: "user" | "assistant";
  content: string;
  timestamp?: string;
}

/**
 * Chunk conversation messages for vector storage
 * Strategy: Message-level chunks with 400-token target, 50-token overlap
 *
 * @param messages Array of conversation messages
 * @param sessionId Conversation session ID
 * @param organizationId Tenant organization ID (CRITICAL for multi-tenancy)
 * @returns Array of LangChain Documents ready for embedding
 */
export async function chunkConversation(
  messages: Message[],
  sessionId: string,
  organizationId: number
): Promise<Document[]> {
  // Format messages: "role: content" (semantic coherence for embeddings)
  const formattedMessages = messages.map((msg, index) => {
    return {
      pageContent: `${msg.role}: ${msg.content}`,
      metadata: {
        organizationId,           // CRITICAL: tenant isolation
        sessionId,
        messageIndex: index,
        role: msg.role,
        timestamp: msg.timestamp || new Date().toISOString(),
        embeddingModel: "text-embedding-3-small",  // Version tracking
      }
    };
  });

  // Create LangChain Documents
  const docs = formattedMessages.map(
    (msg) => new Document({
      pageContent: msg.pageContent,
      metadata: msg.metadata,
    })
  );

  // Split into chunks (400 tokens target, 50 token overlap)
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 400,          // ~100 words, 2-3 messages
    chunkOverlap: 50,        // 12.5% overlap (recommended 10-20%)
    separators: ["\n\n", "\n", " ", ""],  // Preserve semantic boundaries
    lengthFunction: countTokens,          // Use tiktoken for accurate token count
  });

  const chunks = await splitter.splitDocuments(docs);

  return chunks;
}

/**
 * Count tokens using tiktoken (accurate for OpenAI models)
 * Used by RecursiveCharacterTextSplitter to respect 400-token chunk size
 */
function countTokens(text: string): number {
  const encoder = encoding_for_model("gpt-3.5-turbo");  // Same tokenizer as embeddings
  const tokens = encoder.encode(text);
  encoder.free();  // Important: free memory after encoding
  return tokens.length;
}
```

**Why RecursiveCharacterTextSplitter:**
- Preserves semantic boundaries (doesn't cut mid-sentence)
- Chunk overlap maintains context across boundaries
- Length function ensures accurate 400-token chunks (not character count)
- Handles variable-length messages gracefully

**Chunk size rationale (from research):**
- 400 tokens = ~2-3 messages average
- Smaller than 200: Too granular, loses context
- Larger than 800: Too much noise, poor retrieval precision
- 400 = sweet spot for conversational coherence
  </action>
  <verify>
    - packages/server/src/lib/rag/conversationChunker.ts exists
    - Code uses RecursiveCharacterTextSplitter (not manual string splitting)
    - chunkConversation() returns Promise<Document[]>
    - Metadata includes organizationId, sessionId, messageIndex, timestamp, embeddingModel
    - tiktoken used for token counting (not character count)
    - TypeScript compiles without errors
  </verify>
  <done>Conversation chunker created with semantic splitting (400 tokens, 50 overlap) and accurate token counting</done>
</task>

<task type="auto">
  <name>Task 3: Create vector store integration with QdrantVectorStore</name>
  <files>packages/server/src/lib/rag/vectorStore.ts (NEW)</files>
  <action>
Create vectorStore.ts wrapper for QdrantVectorStore operations:

**File: packages/server/src/lib/rag/vectorStore.ts**
```typescript
import { QdrantVectorStore } from "@langchain/qdrant";
import { Document } from "@langchain/core/documents";
import { getEmbeddings } from "./embeddingService";
import { getQdrantClient } from "./qdrantClient";

const COLLECTION_NAME = "chatbot_memory";

/**
 * Get QdrantVectorStore instance connected to chatbot_memory collection
 */
export async function getVectorStore(): Promise<QdrantVectorStore> {
  const embeddings = getEmbeddings();
  const client = getQdrantClient();

  const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
    client: client,
    collectionName: COLLECTION_NAME,
  });

  return vectorStore;
}

/**
 * Store conversation chunks in Qdrant
 * Generates embeddings and upserts to vector database
 *
 * @param chunks Array of LangChain Documents (from conversationChunker)
 * @returns Number of chunks stored
 */
export async function storeConversationChunks(chunks: Document[]): Promise<number> {
  if (chunks.length === 0) {
    return 0;
  }

  const vectorStore = await getVectorStore();

  // QdrantVectorStore.addDocuments handles:
  // 1. Embedding generation (batch via embedDocuments)
  // 2. Vector normalization (Cosine distance)
  // 3. Upsert to Qdrant collection
  await vectorStore.addDocuments(chunks);

  console.log(`[VectorStore] Stored ${chunks.length} conversation chunks`);

  return chunks.length;
}

/**
 * Retrieve relevant conversation chunks using semantic search
 * CRITICAL: Always filters by organizationId (tenant isolation)
 *
 * @param query User's current message (semantic search query)
 * @param organizationId Tenant organization ID
 * @param sessionId Optional: filter to specific conversation
 * @param topK Number of chunks to retrieve (default 5)
 * @returns Array of relevant documents with similarity scores
 */
export async function retrieveRelevantChunks(
  query: string,
  organizationId: number,
  sessionId?: string,
  topK: number = 5
): Promise<Document[]> {
  const vectorStore = await getVectorStore();

  // Build tenant isolation filter (CRITICAL: prevent data leakage)
  const mustFilters: any[] = [
    { key: "organizationId", match: { value: organizationId } }
  ];

  // Optional: filter to specific session
  if (sessionId) {
    mustFilters.push({ key: "sessionId", match: { value: sessionId } });
  }

  // Create retriever with hybrid search (semantic + metadata filtering)
  const retriever = vectorStore.asRetriever({
    k: topK,
    filter: {
      must: mustFilters
    }
  });

  // Retrieve semantically relevant chunks
  const relevantDocs = await retriever.getRelevantDocuments(query);

  console.log(`[VectorStore] Retrieved ${relevantDocs.length} relevant chunks for org ${organizationId}`);

  return relevantDocs;
}
```

**Why QdrantVectorStore wrapper:**
- Abstracts LangChain integration (clean interface for chatbot)
- Enforces organizationId filtering (security by default)
- Handles embedding generation automatically (no manual embed calls)
- Provides semantic retrieval in single function call

**CRITICAL security note:**
- retrieveRelevantChunks ALWAYS includes organizationId filter
- Never skip tenant filtering (GDPR violation, data leakage)
- Validate organizationId matches authenticated user's org
  </action>
  <verify>
    - packages/server/src/lib/rag/vectorStore.ts exists
    - Code uses @langchain/qdrant QdrantVectorStore (not manual Qdrant client calls)
    - storeConversationChunks() accepts Document[] (from chunker)
    - retrieveRelevantChunks() ALWAYS includes organizationId in filter.must
    - TypeScript compiles without errors
  </verify>
  <done>Vector store integration created with LangChain QdrantVectorStore, semantic retrieval, and mandatory tenant filtering</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] embeddingService.ts created with OpenAI text-embedding-3-small integration
- [ ] conversationChunker.ts created with RecursiveCharacterTextSplitter (400 tokens, 50 overlap)
- [ ] vectorStore.ts created with QdrantVectorStore wrapper
- [ ] All 3 files use LangChain libraries (NOT hand-rolled implementations)
- [ ] organizationId included in metadata and retrieval filters (tenant isolation)
- [ ] TypeScript compiles without errors (pnpm build)
- [ ] No manual OpenAI API calls (embedDocuments/embedQuery only)
</verification>

<success_criteria>
- All RAG pipeline components implemented (embeddings, chunking, storage, retrieval)
- LangChain libraries used throughout (no hand-rolled vector operations)
- Multi-tenancy enforced (organizationId in metadata + retrieval filters)
- Code compiles and passes type checks
- Components ready for integration into chatbot endpoint (next plan)
</success_criteria>

<output>
After completion, create `.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-02-SUMMARY.md`:

# Phase 3.8.4 Plan 2: RAG Pipeline Components

**Embedding service, conversation chunking, and vector storage implemented with LangChain**

## Accomplishments

- Embedding service created (OpenAI text-embedding-3-small, batching, rate limit handling)
- Conversation chunker implemented (400-token semantic chunks, 50-token overlap, tiktoken counting)
- Vector store integration built (QdrantVectorStore wrapper, semantic retrieval, tenant filtering)
- All components use LangChain libraries (NOT hand-rolled implementations)
- Multi-tenancy enforced at every layer (organizationId in metadata + filters)

## Files Created/Modified

- **Created:** packages/server/src/lib/rag/embeddingService.ts - OpenAI embeddings singleton
- **Created:** packages/server/src/lib/rag/conversationChunker.ts - Semantic message chunking
- **Created:** packages/server/src/lib/rag/vectorStore.ts - Qdrant vector store wrapper
- **Modified:** packages/server/.env.example - OPENAI_API_KEY documentation

## Decisions Made

1. **text-embedding-3-small** vs ada-002 - 5x cheaper ($0.02 vs $0.10/M tokens), same quality, 1536 dimensions
2. **Message-level chunking** vs full conversation vectors - Better retrieval precision, 400-token sweet spot
3. **LangChain abstractions** vs manual Qdrant calls - Rate limiting, batching, error handling built-in
4. **Mandatory tenant filtering** - organizationId ALWAYS in retrieval filters (GDPR compliance)

## Issues Encountered

[Note any embedding API errors, chunk size optimization challenges, or LangChain integration issues]

## Next Step

Ready for Phase 3.8.4 Plan 3: Integrate RAG retrieval into chatbot endpoint with fallback strategy
</output>
