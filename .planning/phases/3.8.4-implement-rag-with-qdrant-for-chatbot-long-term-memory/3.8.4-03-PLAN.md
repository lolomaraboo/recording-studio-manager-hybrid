---
phase: 3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory
plan: 3
type: execute
---

<objective>
Integrate conditional RAG retrieval into chatbot endpoint - recent context always, semantic search on memory keywords.

Purpose: Enable long-term memory retrieval when users explicitly reference past conversations, while maintaining fast responses for normal questions.
Output: Chatbot uses recent context (15 messages) by default, adds RAG retrieval only when memory keywords detected.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-RESEARCH.md
@.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-01-SUMMARY.md
@.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-02-SUMMARY.md
@packages/server/src/routers/ai.ts

**Previous plans completed:**
- Plan 1: Qdrant Docker deployed, collection created, RAG dependencies installed
- Plan 2: Embedding service, conversation chunker, vector store integration implemented

**Current chatbot implementation (packages/server/src/routers/ai.ts):**
- Lines 67-78: Loads FULL conversation history from PostgreSQL
- Lines 88-91: Sends ALL messages to Claude API
- Lines 81-85: Adds user message to history
- Lines 100-150: Handles Claude response, stores in DB

**Integration strategy - Option 4: Contexte récent + RAG conditionnel**

**Core principle:**
1. **Always load recent context** (15 messages) - Provides conversation continuity
2. **Conditionally add RAG** - Only when user references past memory (keywords detected)
3. **No duplication** - RAG excludes recent messages already loaded

**Memory keyword patterns:**
```typescript
const MEMORY_PATTERNS = {
  explicit: /rappelle|souviens|mentionné|dit/i,           // "rappelle-moi..."
  temporal: /avant|précédemment|dernier|semaine|mois/i,   // "la semaine dernière"
  reference: /premier|deuxième|précédent/i,               // "le premier client"
  question: /quel était|qu'est-ce que j'ai|qui était/i,   // "quel était le nom?"
};
```

**Example behavior:**

**Case 1: Normal question (no memory keywords)**
```
User: "Crée un client nommé Alice"
→ Context: 15 recent messages
→ RAG: NOT triggered (no keywords)
→ Latency: 0ms added
→ Response: Fast, uses recent context
```

**Case 2: Memory reference (keywords detected)**
```
User: "Rappelle-moi le client que j'ai créé la semaine dernière"
→ Context: 15 recent + 5 RAG chunks (old relevant messages)
→ RAG: TRIGGERED (keywords: "rappelle", "semaine dernière")
→ Latency: +200ms (RAG retrieval)
→ Response: Finds "Alice" from week-old conversation
```

**Storage strategy:**
- Store user + assistant messages in Qdrant asynchronously (fire-and-forget)
- Don't block chatbot response on vector storage
- PostgreSQL remains source of truth (Qdrant is retrieval index)

**Critical implementation notes:**
- No performance impact for normal questions (~80% of cases)
- RAG latency (+200ms) only when user needs memory (~20% of cases)
- Backward compatibility: existing sessions work unchanged
- Graceful degradation: Qdrant failures fall back to recent context only
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create conditional memory retrieval service</name>
  <files>packages/server/src/lib/rag/memoryRetriever.ts (NEW)</files>
  <action>
Create memoryRetriever.ts implementing conditional RAG strategy:

**File: packages/server/src/lib/rag/memoryRetriever.ts**
```typescript
import { eq } from "drizzle-orm";
import { aiConversations } from "@rsm/database/tenant";
import { TenantDb } from "@rsm/database/tenant";
import { retrieveRelevantChunks } from "./vectorStore";

interface Message {
  role: "user" | "assistant";
  content: string;
  timestamp?: string;
}

const RECENT_CONTEXT_COUNT = 15;  // Always load last 15 messages
const RAG_TOP_K = 5;               // Retrieve top-5 relevant chunks when triggered

/**
 * Memory keyword patterns - triggers RAG retrieval
 */
const MEMORY_PATTERNS = {
  explicit: /rappelle|souviens|mentionné|dit|parlé/i,           // Explicit memory: "rappelle-moi..."
  temporal: /avant|précédemment|dernier|semaine|mois|hier/i,    // Temporal: "la semaine dernière"
  reference: /premier|deuxième|troisième|précédent/i,           // Ordinal: "le premier client"
  question: /quel était|qu'est-ce que j'ai|qui était/i,         // Memory questions: "quel était le nom?"
};

/**
 * Detect if message requires memory retrieval (RAG)
 * @param message User's current message
 * @returns true if memory keywords detected
 */
function needsMemoryRetrieval(message: string): boolean {
  return Object.values(MEMORY_PATTERNS).some(pattern => pattern.test(message));
}

/**
 * Retrieve conversation context with conditional RAG strategy
 *
 * Strategy:
 * - ALWAYS: Recent context (last 15 messages) for continuity
 * - CONDITIONAL: RAG retrieval (top-5 relevant) if memory keywords detected
 * - NO DUPLICATION: RAG excludes recent messages already loaded
 *
 * @param sessionId Conversation session ID
 * @param currentMessage User's current message (for keyword detection + semantic search)
 * @param organizationId Tenant organization ID (for Qdrant filtering)
 * @param tenantDb Tenant database connection
 * @returns Array of messages for LLM context
 */
export async function retrieveConversationContext(
  sessionId: string,
  currentMessage: string,
  organizationId: number,
  tenantDb: TenantDb
): Promise<Message[]> {
  // Load full conversation from PostgreSQL
  const existingConversations = await tenantDb
    .select()
    .from(aiConversations)
    .where(eq(aiConversations.sessionId, sessionId))
    .limit(1);

  if (existingConversations.length === 0) {
    // New conversation, no history
    return [];
  }

  const allMessages: Message[] = JSON.parse(existingConversations[0].messages || "[]");
  const messageCount = allMessages.length;

  // ALWAYS get recent context (last 15 messages)
  const recentMessages = allMessages.slice(-RECENT_CONTEXT_COUNT);

  console.log(`[MemoryRetriever] Loaded ${recentMessages.length} recent messages (total: ${messageCount})`);

  // Check if memory retrieval needed
  const needsRAG = needsMemoryRetrieval(currentMessage);

  if (!needsRAG) {
    // Normal question: just recent context
    console.log(`[MemoryRetriever] No memory keywords detected, using recent context only`);
    return recentMessages;
  }

  // Memory reference detected: add RAG retrieval
  console.log(`[MemoryRetriever] Memory keywords detected, triggering RAG retrieval`);

  try {
    // Retrieve semantically relevant chunks from OLD messages (exclude recent)
    const relevantDocs = await retrieveRelevantChunks(
      currentMessage,
      organizationId,
      sessionId,
      RAG_TOP_K
    );

    // Extract message indices from relevant chunks
    const relevantIndices = new Set<number>();
    relevantDocs.forEach((doc) => {
      const messageIndex = doc.metadata.messageIndex;
      if (typeof messageIndex === "number") {
        relevantIndices.add(messageIndex);
      }
    });

    // Get indices of recent messages (to avoid duplication)
    const recentIndices = new Set(
      Array.from({ length: RECENT_CONTEXT_COUNT }, (_, i) => messageCount - RECENT_CONTEXT_COUNT + i)
        .filter(i => i >= 0)
    );

    // Filter RAG results: only OLD messages (not in recent context)
    const oldRelevantIndices = Array.from(relevantIndices)
      .filter(i => !recentIndices.has(i))
      .sort((a, b) => a - b);

    const oldRelevantMessages = oldRelevantIndices.map(i => allMessages[i]);

    // Combine: OLD relevant messages + RECENT messages (chronological order)
    const combinedContext = [...oldRelevantMessages, ...recentMessages];

    console.log(
      `[MemoryRetriever] Retrieved ${oldRelevantMessages.length} old relevant + ${recentMessages.length} recent = ${combinedContext.length} total messages`
    );

    return combinedContext;

  } catch (error) {
    // Graceful degradation: fall back to recent context if RAG fails
    console.error("[MemoryRetriever] RAG retrieval failed, falling back to recent context only:", error);
    return recentMessages;
  }
}
```

**Why this strategy works:**

1. **Fast by default** - No RAG latency for normal questions (80% of cases)
2. **Smart retrieval** - RAG triggered only when user needs memory
3. **No duplication** - RAG excludes recent messages (token efficiency)
4. **Graceful degradation** - Qdrant failures don't break chatbot
5. **Simple logic** - Just keyword detection + conditional retrieval

**Keyword pattern rationale:**
- "rappelle-moi le client créé avant" → 3 matches (rappelle, créé, avant)
- "crée un nouveau client" → 0 matches (no memory reference)
- "quel était le premier projet?" → 2 matches (quel était, premier)
  </action>
  <verify>
    - packages/server/src/lib/rag/memoryRetriever.ts exists
    - Code implements conditional RAG (keyword detection with MEMORY_PATTERNS)
    - ALWAYS loads recent context (15 messages)
    - RAG retrieval excludes recent messages (no duplication)
    - Graceful fallback to recent context on error (try/catch)
    - TypeScript compiles without errors
  </verify>
  <done>Conditional memory retrieval service created (recent always + RAG on keywords) with graceful degradation</done>
</task>

<task type="auto">
  <name>Task 2: Integrate conditional RAG into chatbot endpoint (ai.ts)</name>
  <files>packages/server/src/routers/ai.ts</files>
  <action>
Modify ai.ts to use conditional RAG retrieval and store messages in Qdrant:

**Changes to packages/server/src/routers/ai.ts:**

1. **Add imports (top of file):**
```typescript
import { retrieveConversationContext } from "../lib/rag/memoryRetriever";
import { chunkConversation } from "../lib/rag/conversationChunker";
import { storeConversationChunks } from "../lib/rag/vectorStore";
```

2. **Replace conversation history loading (lines 67-78):**

OLD CODE:
```typescript
// Load conversation history if session exists
let conversationHistory: any[] = [];
if (inputSessionId) {
  const existingConversations = await tenantDb
    .select()
    .from(aiConversations)
    .where(eq(aiConversations.sessionId, inputSessionId))
    .limit(1);

  if (existingConversations.length > 0) {
    conversationHistory = JSON.parse(existingConversations[0].messages || "[]");
  }
}
```

NEW CODE:
```typescript
// Load conversation context with conditional RAG strategy
// - Recent context (15 messages) always loaded
// - RAG retrieval added only if memory keywords detected
let conversationHistory: any[] = [];
if (inputSessionId) {
  conversationHistory = await retrieveConversationContext(
    inputSessionId,
    message,
    ctx.organizationId!,
    tenantDb
  );
}
```

3. **Add async vector storage AFTER saving to PostgreSQL (after line ~165):**

Find the code that saves conversation to PostgreSQL:
```typescript
// Save/update conversation in PostgreSQL
if (inputSessionId && existingConversations.length > 0) {
  await tenantDb
    .update(aiConversations)
    .set({ messages: JSON.stringify(conversationHistory), updatedAt: new Date() })
    .where(eq(aiConversations.sessionId, sessionId));
} else {
  await tenantDb.insert(aiConversations).values({
    sessionId,
    messages: JSON.stringify(conversationHistory),
    createdAt: new Date(),
    updatedAt: new Date(),
  });
}
```

ADD AFTER PostgreSQL save (don't await, fire-and-forget):
```typescript
// Store in Qdrant asynchronously (don't block chatbot response)
// Only store last 2 messages (user + assistant from this turn)
const newMessages = conversationHistory.slice(-2).map((m, i) => ({
  role: m.role as "user" | "assistant",
  content: m.content,
  timestamp: m.timestamp || new Date().toISOString(),
}));

// Fire-and-forget: store in background (don't await)
chunkConversation(newMessages, sessionId, ctx.organizationId!)
  .then((chunks) => storeConversationChunks(chunks))
  .then((count) => {
    console.log(`[Chatbot] Stored ${count} chunks in Qdrant for session ${sessionId}`);
  })
  .catch((error) => {
    console.error("[Chatbot] Failed to store in Qdrant:", error);
    // Don't throw - vector storage is non-critical, PostgreSQL is source of truth
  });
```

**CRITICAL implementation notes:**

1. **retrieveConversationContext is smart:**
   - Detects memory keywords in user message
   - Loads recent context (15 msgs) always
   - Adds RAG retrieval conditionally

2. **Fire-and-forget Qdrant storage:**
   - Embedding + vector storage takes 200-500ms
   - User shouldn't wait for indexing
   - Eventual consistency is acceptable

3. **PostgreSQL remains source of truth:**
   - Full conversation still saved to PostgreSQL
   - Qdrant is retrieval index only
   - Error in Qdrant doesn't break chatbot

4. **Backward compatibility:**
   - Existing sessions work unchanged
   - Old messages indexed gradually as users interact
   - No data migration required
  </action>
  <verify>
    - packages/server/src/routers/ai.ts imports RAG modules
    - Conversation history loading replaced with retrieveConversationContext()
    - Qdrant storage added after PostgreSQL save (fire-and-forget pattern)
    - Error handling: Qdrant failures logged but don't break chatbot
    - TypeScript compiles without errors (pnpm build)
  </verify>
  <done>Chatbot endpoint integrated with conditional RAG (recent always + RAG on keywords) and asynchronous vector storage</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Conditional RAG-powered chatbot - recent context always, semantic search triggered by memory keywords</what-built>
  <how-to-verify>
**Test 1: Normal question (NO RAG trigger)**
1. Visit https://recording-studio-manager.com/admin/chat
2. Start new conversation: "Crée un client nommé Alice"
3. Check server logs: "[MemoryRetriever] No memory keywords detected, using recent context only"
4. Expected: Fast response (~2s), no RAG latency
5. Check browser DevTools Network tab: ai.chat response <2.5s

**Test 2: Memory reference (RAG TRIGGERED)**
1. Same conversation, Turn 2: "Rappelle-moi le nom du client que je viens de créer"
2. Check server logs:
   - "[MemoryRetriever] Memory keywords detected, triggering RAG retrieval"
   - "[VectorStore] Retrieved X relevant chunks"
3. Expected: Chatbot responds "Alice" (retrieved via RAG)
4. Latency: ~2.2-2.5s (includes +200ms RAG)

**Test 3: RAG with temporal reference**
1. Turn 3: "Quel était le premier client créé?"
2. Keywords: "quel était" + "premier" → RAG triggered
3. Expected: Chatbot responds "Alice"
4. Check server logs: "[MemoryRetriever] Retrieved X old relevant + 15 recent = Y total messages"

**Test 4: Qdrant storage verification**
1. After Test 1-3, check server logs: "[Chatbot] Stored X chunks in Qdrant"
2. SSH to VPS: ssh root@31.220.104.244
3. Query Qdrant: curl http://localhost:6333/collections/chatbot_memory
4. Expected: "points_count" > 0 (messages indexed)
5. Verify payload: curl http://localhost:6333/collections/chatbot_memory/points/scroll | jq '.result.points[0].payload'
6. Expected: organizationId, sessionId, messageIndex, timestamp fields

**Test 5: Graceful degradation (Qdrant failure)**
1. Stop Qdrant: docker stop qdrant
2. Try chatbot with memory keyword: "Rappelle-moi le client Alice"
3. Check server logs: "[MemoryRetriever] RAG retrieval failed, falling back to recent context only"
4. Expected: Chatbot still responds (uses recent 15 messages)
5. If Alice in recent 15: chatbot remembers
6. If Alice NOT in recent 15: chatbot says "don't recall"
7. Restart Qdrant: docker start qdrant

**Test 6: No regressions**
- Command palette works (Cmd+K)
- AI actions work (create client, schedule session)
- Cache invalidation works (list updates)
- No console errors in browser DevTools
- Chatbot response time for normal questions unchanged (~2s)

**Test 7: Keyword detection accuracy**
- "Crée un client Bob" → NO RAG (no keywords)
- "Avant de créer, vérifie si..." → RAG triggered (keyword "avant")
- "Le dernier projet que j'ai fait" → RAG triggered ("dernier")
- "Qu'est-ce que j'ai dit plus tôt?" → RAG triggered ("qu'est-ce que j'ai")
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] memoryRetriever.ts created with conditional RAG (MEMORY_PATTERNS)
- [ ] ai.ts modified to use retrieveConversationContext()
- [ ] Recent context (15 messages) ALWAYS loaded
- [ ] RAG triggered only on memory keywords
- [ ] Qdrant storage is fire-and-forget (async, non-blocking)
- [ ] Error handling: Qdrant failures don't break chatbot
- [ ] TypeScript compiles (pnpm build)
- [ ] Human verification: Normal questions fast, memory questions use RAG, graceful degradation works
</verification>

<success_criteria>
- Conditional RAG integrated (recent always + RAG on keywords)
- Memory keyword patterns detect user intent correctly
- No latency impact for normal questions (~80% of cases)
- RAG retrieval works for memory references (~20% of cases)
- Vector storage is asynchronous (doesn't block responses)
- Graceful degradation on Qdrant failures
- Existing chatbot features working (no regressions)
- Messages indexed in Qdrant collection (verified via curl)
</success_criteria>

<output>
After completion, create `.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-03-SUMMARY.md`:

# Phase 3.8.4 Plan 3: Conditional RAG Integration

**Smart memory retrieval - recent context always, RAG triggered by keywords**

## Accomplishments

- Conditional memory retrieval service created (MEMORY_PATTERNS keyword detection)
- Chatbot endpoint integrated smart RAG (recent 15 always + RAG on memory keywords)
- Asynchronous vector storage (fire-and-forget, doesn't block response)
- Graceful degradation implemented (Qdrant failures fall back to recent context)
- Zero latency impact for normal questions (80% of cases)
- RAG retrieval only when needed (20% of memory reference cases)

## Files Created/Modified

- **Created:** packages/server/src/lib/rag/memoryRetriever.ts - Conditional RAG strategy
- **Modified:** packages/server/src/routers/ai.ts - Integrated conditional RAG
  - Replaced full history loading with retrieveConversationContext()
  - Added fire-and-forget Qdrant storage after PostgreSQL save
  - Error handling for Qdrant failures (logged, doesn't break chatbot)

## Decisions Made

1. **Recent context: 15 messages always** - Provides conversation continuity without RAG overhead
2. **Conditional RAG trigger** - 4 keyword patterns (explicit, temporal, reference, question)
3. **No duplication** - RAG excludes recent 15 messages (token efficiency)
4. **Fire-and-forget storage** - Qdrant indexing doesn't block chatbot response
5. **PostgreSQL source of truth** - Qdrant is retrieval index only (eventual consistency)

## Memory Keyword Patterns

- **Explicit:** "rappelle", "souviens", "mentionné", "dit", "parlé"
- **Temporal:** "avant", "précédemment", "dernier", "semaine", "mois", "hier"
- **Reference:** "premier", "deuxième", "troisième", "précédent"
- **Question:** "quel était", "qu'est-ce que j'ai", "qui était"

## Performance Impact

- Normal questions (no keywords): **0ms added** (just recent context)
- Memory questions (keywords): **+200ms** (RAG retrieval)
- ~80% questions: No latency impact
- ~20% questions: Acceptable latency for memory retrieval

## Issues Encountered

[Note any keyword false positives/negatives, RAG retrieval quality, or integration challenges]

## Testing Results

- ✅ Normal questions: Fast response, no RAG triggered
- ✅ Memory keywords: RAG triggered, relevant messages retrieved
- ✅ Qdrant storage: Messages indexed asynchronously
- ✅ Graceful degradation: Chatbot works when Qdrant down
- ✅ No regressions: All existing features work
- ✅ Performance: Normal questions unchanged (~2s), memory +0.2s

## Next Step

**Phase 3.8.4 COMPLETE** - Conditional RAG with Qdrant fully integrated and production-ready

Ready for Phase 4: Marketing Foundation (landing page, pricing, demo)
</output>
