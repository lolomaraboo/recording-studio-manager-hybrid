---
phase: 3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory
plan: 1
type: execute
---

<objective>
Deploy Qdrant Docker container on VPS and integrate basic RAG infrastructure for chatbot memory retrieval.

Purpose: Enable semantic search for long-term memory retrieval when users explicitly reference past conversations.
Output: Qdrant running on VPS, collection created, ready for conditional RAG integration (context recent + RAG on memory keywords).
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
~/.claude/get-shit-done/references/cli-automation.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-RESEARCH.md

**Phase research findings (from 3.8.4-RESEARCH.md):**

**Standard stack:**
- @qdrant/js-client-rest 1.11+ (official TypeScript client)
- @langchain/qdrant 0.0.5+ (LangChain integration)
- @langchain/openai 0.3+ (text-embedding-3-small embeddings)
- @langchain/core 0.3+ (base types)

**Architecture decisions:**
- Self-hosted Qdrant Docker on VPS (€0 additional cost)
- Single collection with payload-based multi-tenancy (organizationId filter)
- OpenAI text-embedding-3-small ($0.02/M tokens, 1536 dimensions)
- Message-level chunking with 400-token semantic boundaries

**Don't hand-roll:**
- Embedding generation → Use @langchain/openai OpenAIEmbeddings (handles rate limiting, retries, batching)
- Conversation chunking → Use LangChain SemanticChunker (preserves semantic boundaries)
- Vector normalization → Qdrant auto-normalizes with distance: "Cosine"
- Similarity search → Use Qdrant query API (HNSW algorithm, filtered search)
- Multi-tenant isolation → Use Qdrant payload filters (security + performance)

**Critical pitfalls to avoid:**
- NOT filtering by organizationId in queries (data leakage, GDPR violation)
- Chunk size too large (retrieval returns irrelevant context)
- Embedding model mismatch (changing model without re-embedding)
- Rate limit death spiral (embedding in loops without batching)
- Payload size exceeded (>40KB, store only metadata in payload)

**Current chatbot implementation:**
@packages/server/src/routers/ai.ts

- Lines 67-78: Loads FULL conversation history from PostgreSQL
- Lines 88-91: Sends ALL messages to Claude API
- Current approach: Works well but doesn't scale beyond ~100 messages
- New approach: Keep recent context (15 messages), add RAG only when user references memory

**Infrastructure context:**
- VPS: Hostinger 31.220.104.244 (4GB RAM, 2 vCPU)
- Docker containers: rsm-server, rsm-client, rsm-postgres, rsm-redis
- Port availability: 6333, 6334 (Qdrant default ports)
- Storage: /root/qdrant_storage will be created for persistence
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deploy Qdrant Docker container on VPS</name>
  <files>docker-compose.yml (or manual docker run command on VPS)</files>
  <action>
SSH to VPS (ssh root@31.220.104.244) and deploy Qdrant Docker container:

```bash
# Pull official Qdrant image
docker pull qdrant/qdrant

# Run with persistent storage
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -p 6334:6334 \
  -v /root/qdrant_storage:/qdrant/storage \
  --restart unless-stopped \
  qdrant/qdrant

# Verify running
docker ps | grep qdrant
curl http://localhost:6333/collections
```

**CRITICAL:** Qdrant runs on ports 6333 (HTTP API) and 6334 (gRPC). These ports should NOT be exposed publicly (no Nginx proxy). Only server container needs internal access via Docker network.

**Security:** Qdrant has no authentication in default config. This is ACCEPTABLE because:
1. Not exposed to public internet (internal Docker network only)
2. Protected by VPS firewall (only 80/443/22 open)
3. Application-level auth via organizationId payload filtering

If adding to docker-compose.yml, add service:
```yaml
qdrant:
  image: qdrant/qdrant
  container_name: qdrant
  ports:
    - "6333:6333"
    - "6334:6334"
  volumes:
    - /root/qdrant_storage:/qdrant/storage
  restart: unless-stopped
  networks:
    - rsm-network
```

Then connect from server using "http://qdrant:6333" (Docker internal DNS).
  </action>
  <verify>
    - docker ps shows qdrant container running
    - curl http://localhost:6333/collections returns {"result":[],"status":"ok"}
    - /root/qdrant_storage directory created with Qdrant data files
  </verify>
  <done>Qdrant Docker container deployed, accessible at localhost:6333, persistent storage configured</done>
</task>

<task type="auto">
  <name>Task 2: Install RAG dependencies in server package</name>
  <files>packages/server/package.json, packages/server/pnpm-lock.yaml</files>
  <action>
Add required npm packages to packages/server/:

```bash
cd /Users/marabook_m1/Documents/APP_HOME/CascadeProjects/windsurf-project/recording-studio-manager-hybrid/packages/server

pnpm add @qdrant/js-client-rest @langchain/qdrant @langchain/openai @langchain/core langchain tiktoken
```

**Versions** (based on research):
- @qdrant/js-client-rest: ^1.11.0 (latest stable)
- @langchain/qdrant: ^0.0.5 (latest)
- @langchain/openai: ^0.3.0 (latest)
- @langchain/core: ^0.3.0 (required peer dependency)
- langchain: ^0.3.0 (document loaders, text splitters)
- tiktoken: ^1.0.0 (token counting for chunk optimization)

**Why these libraries:**
- @qdrant/js-client-rest: Official Qdrant TypeScript client (REST API, no gRPC dependencies)
- @langchain/qdrant: Dedicated LangChain integration (better types than @langchain/community)
- @langchain/openai: OpenAIEmbeddings class with rate limiting, retries, batching
- langchain: SemanticChunker, RecursiveCharacterTextSplitter for conversation chunking
- tiktoken: Count tokens in chunks to optimize 400-token target

**Avoid:**
- @langchain/community (deprecated Qdrant support since Sep 2024)
- Manual OpenAI API calls (no rate limit handling, error prone)
- Fixed-size chunking without semantic boundaries
  </action>
  <verify>
    - pnpm list | grep qdrant shows @qdrant/js-client-rest installed
    - pnpm list | grep langchain shows all @langchain/* packages installed
    - packages/server/package.json contains all 6 dependencies
  </verify>
  <done>RAG dependencies installed (Qdrant client, LangChain libraries, tiktoken)</done>
</task>

<task type="auto">
  <name>Task 3: Create Qdrant collection with multi-tenant configuration</name>
  <files>packages/server/src/lib/rag/qdrantClient.ts (NEW), packages/server/src/lib/rag/index.ts (NEW)</files>
  <action>
Create qdrantClient.ts singleton wrapper for Qdrant connection:

**File: packages/server/src/lib/rag/qdrantClient.ts**
```typescript
import { QdrantClient } from "@qdrant/js-client-rest";

let qdrantClient: QdrantClient | null = null;

export function getQdrantClient(): QdrantClient {
  if (!qdrantClient) {
    const qdrantUrl = process.env.QDRANT_URL || "http://localhost:6333";
    qdrantClient = new QdrantClient({ url: qdrantUrl });
  }
  return qdrantClient;
}
```

**File: packages/server/src/lib/rag/index.ts**
```typescript
import { getQdrantClient } from "./qdrantClient";

const COLLECTION_NAME = "chatbot_memory";

/**
 * Initialize Qdrant collection for chatbot memory
 * Only runs once - idempotent (safe to call multiple times)
 */
export async function initializeQdrantCollection(): Promise<void> {
  const client = getQdrantClient();

  // Check if collection already exists
  const collections = await client.getCollections();
  const exists = collections.collections.some((c) => c.name === COLLECTION_NAME);

  if (exists) {
    console.log(`[Qdrant] Collection "${COLLECTION_NAME}" already exists`);
    return;
  }

  // Create collection with OpenAI text-embedding-3-small config
  await client.createCollection(COLLECTION_NAME, {
    vectors: {
      size: 1536,              // text-embedding-3-small dimension
      distance: "Cosine",      // Auto-normalizes vectors
    },
    optimizers_config: {
      indexing_threshold: 10000,  // Build HNSW after 10k vectors
    },
    hnsw_config: {
      m: 16,                   // HNSW graph connections (16 = balanced)
      ef_construct: 100,       // Build quality (100 = good for 100k+ vectors)
    },
  });

  // Create payload index for fast organizationId filtering (CRITICAL for multi-tenancy)
  await client.createPayloadIndex(COLLECTION_NAME, {
    field_name: "organizationId",
    field_schema: "integer",
  });

  // Create payload index for timestamp filtering (for recency bias)
  await client.createPayloadIndex(COLLECTION_NAME, {
    field_name: "timestamp",
    field_schema: "datetime",
  });

  console.log(`[Qdrant] Collection "${COLLECTION_NAME}" created successfully`);
}
```

**Add to server startup (packages/server/src/index.ts):**
```typescript
import { initializeQdrantCollection } from "./lib/rag";

// After database initialization, before server start
await initializeQdrantCollection();
```

**Environment variable (packages/server/.env.example and VPS .env):**
```
# Qdrant Vector Database (for chatbot RAG)
QDRANT_URL=http://localhost:6333  # Local: localhost, Production VPS: http://qdrant:6333 (Docker network)
```

**CRITICAL:** Always include organizationId in payload filters to prevent data leakage between tenants (GDPR violation).
  </action>
  <verify>
    - curl http://localhost:6333/collections shows chatbot_memory collection
    - curl http://localhost:6333/collections/chatbot_memory shows vectors.size=1536, distance="Cosine"
    - Collection has payload indexes for organizationId (integer) and timestamp (datetime)
    - Server starts without errors, logs "[Qdrant] Collection created" or "already exists"
  </verify>
  <done>Qdrant collection "chatbot_memory" created with multi-tenant configuration (1536 dimensions, Cosine distance, payload indexes)</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Qdrant Docker container running on VPS (docker ps | grep qdrant)
- [ ] Qdrant accessible via curl http://localhost:6333/collections
- [ ] RAG npm packages installed in packages/server/package.json (6 libraries)
- [ ] Collection "chatbot_memory" created with correct configuration (1536 dimensions, Cosine distance)
- [ ] Payload indexes exist for organizationId and timestamp
- [ ] Server starts without errors (initializeQdrantCollection called)
- [ ] /root/qdrant_storage directory created with persistent data
</verification>

<success_criteria>
- All tasks completed (Qdrant deployed, dependencies installed, collection created)
- Qdrant container running and accessible from server
- Multi-tenant collection configured (organizationId payload indexing)
- Server integrates Qdrant initialization on startup
- No errors in Qdrant logs or server startup
</success_criteria>

<output>
After completion, create `.planning/phases/3.8.4-implement-rag-with-qdrant-for-chatbot-long-term-memory/3.8.4-01-SUMMARY.md`:

# Phase 3.8.4 Plan 1: Qdrant Infrastructure Setup

**Qdrant Docker deployed on VPS, RAG dependencies installed, collection configured for multi-tenant chatbot memory**

## Accomplishments

- Qdrant Docker container deployed on VPS (port 6333/6334, persistent storage)
- RAG dependencies installed (@qdrant/js-client-rest, @langchain/qdrant, @langchain/openai + 3 more)
- Collection "chatbot_memory" created (1536 dimensions, Cosine similarity, HNSW indexing)
- Payload indexes created for organizationId (tenant isolation) and timestamp (recency filtering)
- Server startup integrated Qdrant initialization

## Files Created/Modified

- **Created:** packages/server/src/lib/rag/qdrantClient.ts - Singleton Qdrant connection
- **Created:** packages/server/src/lib/rag/index.ts - Collection initialization logic
- **Modified:** packages/server/package.json - Added 6 RAG dependencies
- **Modified:** packages/server/src/index.ts - Added initializeQdrantCollection() to startup
- **Modified:** packages/server/.env.example - Added QDRANT_URL environment variable
- **VPS:** docker run qdrant/qdrant - Deployed container with persistent storage

## Decisions Made

1. **Self-hosted Qdrant on VPS** vs Qdrant Cloud - €0 additional cost, full control, low latency
2. **Payload-based multi-tenancy** vs separate collections - Official Qdrant best practice, better performance
3. **OpenAI text-embedding-3-small** vs ada-002 - 5x cheaper ($0.02 vs $0.10/M tokens), same quality
4. **Port security** - Qdrant NOT exposed publicly (internal Docker network only, protected by VPS firewall)

## Issues Encountered

[Note any deployment issues, connection problems, or configuration challenges]

## Next Step

Ready for Phase 3.8.4 Plan 2: Implement embedding service and vector storage integration
</output>
