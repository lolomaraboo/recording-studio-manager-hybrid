# Phase 3.8.4 Plan 1: Qdrant Infrastructure Setup

**Qdrant Docker deployed on VPS, RAG dependencies installed, collection configured for multi-tenant chatbot memory**

## Accomplishments

- Qdrant Docker container verified running on VPS (port 6333/6334, persistent storage via Docker volume)
- RAG dependencies installed: @qdrant/js-client-rest ^1.16.2, @langchain/qdrant ^1.0.1, @langchain/openai ^1.2.0, @langchain/core ^1.1.8, langchain ^1.2.3, tiktoken ^1.0.22
- Collection "chatbot_memory" created successfully with:
  - 1536 dimensions (text-embedding-3-small)
  - Cosine similarity distance
  - HNSW indexing (m=16, ef_construct=100, indexing_threshold=10000)
- Payload indexes created for:
  - organizationId (integer) - CRITICAL for multi-tenant isolation
  - timestamp (datetime) - for recency-based filtering
- Server startup integrated with Qdrant initialization (graceful error handling)

## Files Created/Modified

- **Created:** packages/server/src/lib/rag/qdrantClient.ts - Singleton Qdrant connection manager
- **Created:** packages/server/src/lib/rag/index.ts - Collection initialization logic with idempotent creation
- **Modified:** packages/server/package.json - Added 6 RAG dependencies
- **Modified:** packages/server/src/index.ts - Added initializeQdrantCollection() to server startup (after Redis, before middleware)
- **Modified:** packages/server/.env.example - Added QDRANT_URL environment variable
- **Modified:** docker-compose.yml - Added Qdrant service configuration (commented out, using shared instance)
- **VPS:** Verified existing Qdrant container (mem0-api project, port 6333 exposed)
- **VPS:** Added QDRANT_URL=http://localhost:6333 to /root/recording-studio-manager-hybrid/.env

## Decisions Made

1. **Use existing Qdrant container** vs create dedicated instance
   - Rationale: Qdrant already running on VPS from mem0-api project
   - Qdrant supports multiple collections natively (multi-tenant by design)
   - No additional resource overhead
   - Collection isolation provides sufficient separation

2. **Payload-based multi-tenancy** vs separate collections per tenant
   - Rationale: Official Qdrant best practice for multi-tenant applications
   - Better performance (single HNSW graph, payload filtering at query time)
   - Simpler management (one collection vs hundreds)
   - CRITICAL: organizationId payload index ensures fast tenant isolation

3. **OpenAI text-embedding-3-small** vs ada-002
   - Rationale: 5x cheaper ($0.02/M vs $0.10/M tokens), same quality
   - 1536 dimensions (same as ada-002)
   - Better performance for semantic search

4. **Port security** - Qdrant exposed on 0.0.0.0:6333 (VPS firewall protection)
   - Rationale: VPS firewall blocks all ports except 80/443/22
   - Docker containers access via localhost or Docker network
   - No authentication needed (network-level security sufficient)

5. **Graceful startup error handling**
   - Rationale: Server should continue if Qdrant unavailable
   - Logs warning but doesn't crash
   - RAG features disabled until Qdrant available

## Issues Encountered

### Issue 1: Shared Qdrant Instance Discovery
- **Problem:** Found existing Qdrant container from mem0-api project
- **Solution:** Decided to reuse existing container with separate collection
- **Learning:** Always check for existing infrastructure before deploying new containers

### Issue 2: Docker Network Configuration
- **Problem:** RSM containers need to access Qdrant from mem0-api project
- **Solution:** Added host.docker.internal:host-gateway to server's extra_hosts
- **Configuration:** QDRANT_URL=http://host.docker.internal:6333 (from Docker) or http://localhost:6333 (from host)

## Verification Results

- Qdrant container status: RUNNING (6 weeks uptime, restarted 7 hours ago)
- Collection created: chatbot_memory
- Collection config verified:
  - vectors.size: 1536 ✓
  - vectors.distance: Cosine ✓
  - hnsw_config.m: 16 ✓
  - hnsw_config.ef_construct: 100 ✓
  - optimizer_config.indexing_threshold: 10000 ✓
- Payload indexes verified:
  - organizationId: integer, 0 points ✓
  - timestamp: datetime, 0 points ✓
- Collection status: green, optimizer_status: ok ✓
- Server startup test: Pending (will verify when server restarts)

## Next Steps

Ready for Phase 3.8.4 Plan 2: Implement embedding service and vector storage integration

**Tasks for Plan 2:**
1. Create EmbeddingService class (OpenAI text-embedding-3-small)
2. Create VectorStore class (Qdrant upsert/search with organizationId filtering)
3. Implement conversation chunking (semantic boundaries, 400-token target)
4. Add background job for embedding existing conversations
5. Update AI router to conditionally use RAG when user references memory

**Prerequisites met:**
- Qdrant infrastructure deployed ✓
- RAG dependencies installed ✓
- Multi-tenant collection configured ✓
- Environment variables set ✓
